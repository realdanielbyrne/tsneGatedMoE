s% Use the llncs.cls formatting
\documentclass{llncs}

% Set the packages for use within the document. The following
% packages should be included.  Additional packages that do not
% conflict with these packages or change the llncs class formatting
% may be used.  Packages that do change the formatting are
% not allowed.late
\usepackage{graphicx} % Used for displaying a sample figure.
% If possible, figure files should be included in EPS format.
% PDF format is also acceptable. JPEG  will work, but some of
% them are downsampled resulting in fuzzy images.
\usepackage{booktabs} % Better horizontal rules in tables
\usepackage{multirow} % Better combined rows in tables

% The title of the paper
\title{Sparse Mixture of Sparse Experts with t-SNE Feature Selector and Distant Links }

% The complete list of authors with their affiliations
\author{
Daniel Byrne\inst{1} \and
John Santerre\inst{2}
}

\institute{
Master of Science in Data Science,
Southern Methodist University,
Dallas TX 7527, USA
\email{byrned@smu.edu}

\and
PhD in Data Science
Southern Methodist University,
Dallas TX 75275, USA
\email{santerrej@smu.edu}
}


% Begin the document
\begin{document}
\maketitle
\setcounter{footnote}{0}

\begin{abstract}
This paper presents a new semi-supervised learning technique which combines
the dimensionality reduction technique of Parametric t-SNE to gate a
collection of sparse interconnected collection of clustered Feed Forward Sub-Networks.
Parametric t-SNE creates a low dimensional map of training data which will then be
used to gate a layer of feed forward “Expert” networks whose output is a sparse
n-dimensional vector.  The output layer is trained using a cross entropy loss function
and classes are identified with logistic regression classifiers.  The benefits of this
novel approach allows for encoding similar classification characteristics in
physically close proximity.  Implementing  a sparse versus fully connected layer
limits the number of calculable parameters, and allowing for weak interactions across
experts allows for relevant information to be shared but not to dominate or
infiltrate distant experts with noise.




% Keywords may be used, but they are not required.
\keywords{MOE  \and t-SNE \and Sparse \and DNN}
\end{abstract}

% Sections are denoted by the use of the \section{Section Name}
% command -- where "Section Name" is the name you give to the Section.
\section{Introduction}

% Note that paragraphs are created by placing a blank line before the
% paragraph within the .tex file just as a blank line exists before the
% beginning of this comment. That blank line tells LaTeX to treat the
% following text as a new paragraph.  No other commands are needed.

Deep learning’s explosive growth in learning over the past few years has been field
by the technique’s success at embedding high dimensional data into multilevel encoded
representations of increasing abstraction to solve complex functions and or to model
natural processes  such as Natural Language Processing (NLP) and Computer Vision, and
Speech Recognition.

Deep Neural Networks DNNs learned embeddings are typically trained from labeled data by
implementing a learning procedure utilizing a variant of gradient descent modulated by a
learning rate to iteratively descend a generally convex polynomial order objective
function to a learned representation.

In common feed forward DNNs, neurons from the input to the output layers are typically
fully connected.  That is every node at any one layer is connected to every other node
in adjacent layers.  This fully connected nature supplies each successive layer with any
and all embeddings of its preceding layer in any possible combination. It is then the
task of the training procedure to effectively reduce this over abundance of connections
by iteratively applying the delta rule to adjust the weights on the connections.  These
learned weights then. act as a gating threshold which activates or deactivates the neurons
they are connected.  The net result is a collection of weights and neurons  that when
applied an input stimulus effectively recreates an alternative representation or
classification of the input data at the output of the network.

There has been great successes achieved by researchers utilizing DNN architectures by
increasing of both the amount of labeled training data and the number of layers and
thus parameters in the model.   In general it has been shown that with sufficiently
large datasets, increasing the number of parameters in DNNs can give much better
prediction accuracy.

However, training such models is time consuming compute resource intensive.  The
difficulty in training deep neural networks begins and ends with the fact that the
design principle on which many are founded, full connectivity, contributes to a quadratic
increase in calculable parameters as the number of input dimensions and parameters
increases.  In a typical DNN, there may be millions of parameters and similarly large
set of labelled training examples.

In a multi-classification problem, competing classifications with similar and dissimilar
features will inject noise relative to each competing target classification. For example
the theoretical ideal set of weights that consistently identifies cats in images is most
likely in conflict with set of all possible weights of connections tuned to correctly
classify trees.  There will be some overlap, and that overlapping region is non-linear
objective function the neural network is trying to optimize.  However, arriving at one
of the many possible number of near optimal solutions to this equation is not trivial.

There have been a number of solutions to address this problem in the literature.  The
techniques vary, but the end results are typically lower pass filtering noise across the
network.

Dropout is a method by which a tuneable parameter p, often 50, of random hidden units and
their connections are eliminated from the network with each training case.  During test
all available neurons are included, but the trained weights on their connections are
multiplied by the dropout probability p.    Dropout gives major improvements in training
time and accuracy by encouraging each individual neurons to learn a feature by degrading
or filtering the impact other hidden units have on its connection weights.

DropConnect is similar to Dropout in that it introduces dynamic sparsity within the
model, but instead of dropping neurons and their connections, it drops only connections
based again on a settable probability parameter.

Embedding auto-encoders have been successful in image noise reduction and modeling the
time series relationships of stock trading data.  Auto encoders work by compressing
input data of n dimensions into a smaller p dimensional hidden layer which is then
re-encoded into an n dimensional output layer through gradient descent training.

In Convolutional Neural Networks,  pooling layers are often utilized to reduce the size
of the feature maps coming out of a filter bank before feeding the resultant filtered
feature map to successive layers.  This has the effect of reducing the number of
interfering connections, reducing the computational complexity, and low pass filtering
noisy features.

While Dropout and DropConnect reducing the number of parameters during training, but
reinstate them during testing, a true reduction in the network connections has not been
achieved.   Song Han et al. chose to learn both the weights on connections and the
existence of connections during training. Their method first trains the network, prunes
low weight connections, and then retrains the network to fine tune the remaining connections.

Deep Compression has achieved compression rations of 98\% with no loss of accuracy
following a similar process, but in the final step they used Huffman encoding to further
reduce the size of the network.

Inspired by research on Rat Brain Tissue which found that synaptic formation for specific
functional brain regions can be modeled as a random formation, StochasticNets modified
this above approach by starting with a sparse randomly connected network and ending with
a sparse network.   So the pruning is inherent in the design.
Decebal Constantin Mocanu et al. argue that  DNN should never have fully connected layers
contrary general practice, and demonstrate the effectiveness of sparse representations on
several diverse neural network architectures (RBMs, MLPs, and CNNs).

While reducing the number of interfering connections has been shown to be an effective
compression and regularization technique, the problem with DNNs tasked with classifying
numerous items is really a function of graph theory, local specializations, and near and
far influences.

Frankle and Carbin (2019) found out that fine-tuning the weights after training was not
required for pruned networks.  They suggest that the structure of the pruned network is
the determining factor.

There is a biological corollary to this approach as well.  Certain pathways in the brain
have repeating sparse connectivity subnetworks, motifs, that define their architecture.
These repeating structures were also found in other real world networks include
statistically significant subnetworks. Research has shown many complex biological,
social and technical networks naturally form networks which tend to follow a logical,
locally spatial dense connectivity with sparse regional connectivity between motifs.
This local preference with weak connections with distant modules is repeated at many
different macro levels appears to be a powerful modeling technique.

Thus that leads to the assumption that while sparse networks tend to out perform dense
ones, perhaps the networks we should be trying to develop are composite networks
comprised of a mixture of local experts combining to solve a larger problem or set of problems.

Such technique have also beeb implemented by Hinton et al. in his Adaptive Mixtures of
Local Experts and sparsely gated mixture of experts model.  These models using a separate
gating network which permits access to segregated subnetworks in a feed forward neural
network by selectively activating or deactivating access gates.  These gates and the
feedforward networks in them are trained to engage  a Mixture of Expert (MoE) to solve
the objective function in question.

In this paper we propose a modification of this MOE model in which the gating neurons are
trained using a Parametric T-SNE loss function. Parametric T-Stochastic Neighbor Embedding
(T-SNE) is a method by which the Mutual Information component of similar Gaussian
distributions are converted into a Euclidian distances to reduce a high dimensional signal
into a lower dimensional space while preserving the structure of the data.

Parametric T-SNE is used to gate the MoE FF networks so that spatial clusters can form
around similar structures.  Furthermore, allowing t-SNE to drive the gates allows us embed
similarities in training data samples on top of the labels already attached to the data
thus effectively applying multiple classifications to our data without any added effort.

Subnetworks are sparsely initialized using methods devised in StochasticNet, and owing to
the the potential influence of far flung experts on local problems, minimal connections
in-between expert subnetworks are permitted albeit at a smaller randomly assigned rate.

The MoE FF networks are then trained with gradient descent independent of the parametric
T-SNE Gate Training.

The result is a mixture of experts model which is spatially aware, embeds classifications
beyond that which can be calculated with the and which allows for contributions from far
flung expert networks through minimal influence connections.

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted according to alphabetical
% and formatted in the correct style.
%
 \bibliographystyle{splncs04}

 \bibliography{samplebib}

% End the document
\end{document}
