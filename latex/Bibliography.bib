%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Daniel at 2020-04-14 00:03:17 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@article{doi:10.1162/neco.1997.9.1.185,
	Abstract = { This article proposes the use of a penalty function for pruning feedforward neural network by weight elimination. The penalty function proposed consists of two terms. The first term is to discourage the use of unnecessary connections, and the second term is to prevent the weights of the connections from taking excessively large values. Simple criteria for eliminating weights from the network are also given. The effectiveness of this penalty function is tested on three well-known problems: the contiguity problem, the parity problems, and the monks problems. The resulting pruned networks obtained for many of these problems have fewer connections than previously reported in the literature. },
	Author = {Setiono, Rudy},
	Date-Added = {2020-04-14 00:03:02 -0500},
	Date-Modified = {2020-04-14 00:03:02 -0500},
	Doi = {10.1162/neco.1997.9.1.185},
	Eprint = {https://doi.org/10.1162/neco.1997.9.1.185},
	Journal = {Neural Computation},
	Number = {1},
	Pages = {185-204},
	Title = {A Penalty-Function Approach for Pruning Feedforward Neural Networks},
	Url = {https://doi.org/10.1162/neco.1997.9.1.185},
	Volume = {9},
	Year = {1997},
	Bdsk-Url-1 = {https://doi.org/10.1162/neco.1997.9.1.185}}

@book{Kingma:2015aa,
	Author = {Diederik P. Kingma},
	Date-Added = {2020-04-13 23:54:46 -0500},
	Date-Modified = {2020-04-13 23:54:46 -0500},
	Title = {Adam: A Method for Stochastic Optimization.; ICLR (Poster)},
	Url = {http://arxiv.org/abs/1412.6980},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1412.6980}}

@article{Louizos2017LearningSN,
	Author = {Christos Louizos and Max Welling and Diederik P. Kingma},
	Date-Added = {2020-04-13 23:14:10 -0500},
	Date-Modified = {2020-04-13 23:14:10 -0500},
	Journal = {ArXiv},
	Title = {Learning Sparse Neural Networks through L0 Regularization},
	Volume = {abs/1712.01312},
	Year = {2017}}

@article{Hershey_2007,
	Author = {Hershey, John R. and Olsen, Peder A.},
	Date-Added = {2020-04-13 23:11:42 -0500},
	Date-Modified = {2020-04-13 23:11:42 -0500},
	Doi = {10.1109/icassp.2007.366913},
	Isbn = {1424407273},
	Journal = {2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07},
	Month = {Apr},
	Publisher = {IEEE},
	Title = {Approximating the Kullback Leibler Divergence Between Gaussian Mixture Models},
	Url = {http://dx.doi.org/10.1109/ICASSP.2007.366913},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICASSP.2007.366913},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/icassp.2007.366913}}

@article{Mitchell_1988,
	Author = {Mitchell, T. J. and Beauchamp, J. J.},
	Date-Added = {2020-04-13 23:11:41 -0500},
	Date-Modified = {2020-04-13 23:11:41 -0500},
	Doi = {10.1080/01621459.1988.10478694},
	Issn = {1537-274X},
	Journal = {Journal of the American Statistical Association},
	Month = {Dec},
	Number = {404},
	Pages = {1023--1032},
	Publisher = {Informa UK Limited},
	Title = {Bayesian Variable Selection in Linear Regression},
	Url = {http://dx.doi.org/10.1080/01621459.1988.10478694},
	Volume = {83},
	Year = {1988},
	Bdsk-Url-1 = {http://dx.doi.org/10.1080/01621459.1988.10478694}}

@article{Caswell:2016aa,
	Author = {Caswell, Isaac and Shen, Chuanqi and Wang, Lisa},
	Date-Added = {2019-10-07 03:14:13 -0500},
	Date-Modified = {2019-10-07 03:14:13 -0500},
	Journal = {Tech. Report},
	Title = {Loopy neural nets: Imitating feedback loops in the human brain},
	Ty = {JOUR},
	Year = {2016}}

@inproceedings{Krizhevsky:2011aa,
	Author = {Krizhevsky, Alex and Hinton, Geoffrey E},
	Date-Added = {2019-10-07 03:11:10 -0500},
	Date-Modified = {2019-10-07 03:11:10 -0500},
	Journal = {ESANN},
	Pages = {2},
	Title = {Using very deep autoencoders for content-based image retrieval.},
	Ty = {CONF},
	Volume = {1},
	Year = {2011}}

@article{Sporns:2004aa,
	Author = {Sporns, Olaf and K{\"o}tter, Rolf},
	Date-Added = {2019-10-07 03:06:08 -0500},
	Date-Modified = {2019-10-07 03:06:08 -0500},
	Isbn = {1545-7885},
	Journal = {PLoS biology},
	Number = {11},
	Pages = {e369},
	Publisher = {Public Library of Science},
	Title = {Motifs in brain networks},
	Ty = {JOUR},
	Volume = {2},
	Year = {2004}}

@inproceedings{Kiyono:2019aa,
	Author = {Kiyono, Shun and Suzuki, Jun and Inui, Kentaro},
	Date-Added = {2019-10-07 03:03:53 -0500},
	Date-Modified = {2019-10-07 03:03:53 -0500},
	Isbn = {2374-3468},
	Journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	Pages = {4073--4081},
	Title = {Mixture of expert/imitator networks: Scalable semi-supervised learning framework},
	Ty = {CONF},
	Volume = {33},
	Year = {2019}}

@inproceedings{Hassibi:1993aa,
	Author = {Hassibi, Babak and Stork, David G and Wolff, Gregory J},
	Date-Added = {2019-10-07 03:01:19 -0500},
	Date-Modified = {2019-10-07 03:01:19 -0500},
	Isbn = {0780309995},
	Journal = {IEEE international conference on neural networks},
	Pages = {293--299},
	Publisher = {IEEE},
	Title = {Optimal brain surgeon and general network pruning},
	Ty = {CONF},
	Year = {1993}}

@inproceedings{LeCun:1990aa,
	Author = {LeCun, Yann and Denker, John S and Solla, Sara A},
	Date-Added = {2019-10-07 03:00:27 -0500},
	Date-Modified = {2019-10-07 03:00:27 -0500},
	Journal = {Advances in neural information processing systems},
	Pages = {598--605},
	Title = {Optimal brain damage},
	Ty = {CONF},
	Year = {1990}}

@inproceedings{Zorins:2015aa,
	Author = {Zorins, Aleksejs and Grabusts, Peter},
	Date-Added = {2019-10-07 03:00:04 -0500},
	Date-Modified = {2019-10-07 03:00:04 -0500},
	Journal = {Proceedings of the 10th International Scientific and Practical Conference. Volume III},
	Pages = {231},
	Title = {Artificial Neural Networks and Human Brain: Survey of Improvement Possibilities of Learning},
	Ty = {CONF},
	Volume = {228},
	Year = {2015}}

@article{Samsonovich:2005aa,
	Author = {Samsonovich, Alexei V and Ascoli, Giorgio A},
	Date-Added = {2019-10-07 02:58:30 -0500},
	Date-Modified = {2019-10-07 02:58:30 -0500},
	Isbn = {1072-0502},
	Journal = {Learning \& Memory},
	Number = {2},
	Pages = {193--208},
	Publisher = {Cold Spring Harbor Lab},
	Title = {A simple neural network model of the hippocampus suggesting its pathfinding role in episodic memory retrieval},
	Ty = {JOUR},
	Volume = {12},
	Year = {2005}}

@article{Morris:2003aa,
	Author = {Morris, Genela and Nevet, Alon and Bergman, Hagai},
	Date-Added = {2019-10-07 02:57:47 -0500},
	Date-Modified = {2019-10-07 02:57:47 -0500},
	Isbn = {0928-4257},
	Journal = {Journal of Physiology-Paris},
	Number = {4-6},
	Pages = {581--589},
	Publisher = {Elsevier},
	Title = {Anatomical funneling, sparse connectivity and redundancy reduction in the neural networks of the basal ganglia},
	Ty = {JOUR},
	Volume = {97},
	Year = {2003}}

@article{Watts:1998aa,
	Author = {Watts, Duncan J and Strogatz, Steven H},
	Date-Added = {2019-10-07 02:56:10 -0500},
	Date-Modified = {2019-10-07 02:56:10 -0500},
	Isbn = {1476-4687},
	Journal = {nature},
	Number = {6684},
	Pages = {440},
	Publisher = {Nature Publishing Group},
	Title = {Collective dynamics of `small-world'networks},
	Ty = {JOUR},
	Volume = {393},
	Year = {1998}}

@article{Han:2015aa,
	Author = {Han, Song and Mao, Huizi and Dally, William J},
	Date-Added = {2019-10-07 02:55:01 -0500},
	Date-Modified = {2019-10-07 02:55:01 -0500},
	Journal = {arXiv preprint arXiv:1510.00149},
	Title = {Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
	Ty = {JOUR},
	Year = {2015}}

@inproceedings{Huang:2017aa,
	Author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
	Date-Added = {2019-10-07 02:54:12 -0500},
	Date-Modified = {2019-10-07 02:54:12 -0500},
	Journal = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	Pages = {4700--4708},
	Title = {Densely connected convolutional networks},
	Ty = {CONF},
	Year = {2017}}

@article{Srivastava:2014aa,
	Author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	Date-Added = {2019-10-07 02:53:43 -0500},
	Date-Modified = {2019-10-07 02:53:43 -0500},
	Isbn = {1532-4435},
	Journal = {The journal of machine learning research},
	Number = {1},
	Pages = {1929--1958},
	Publisher = {JMLR. org},
	Title = {Dropout: a simple way to prevent neural networks from overfitting},
	Ty = {JOUR},
	Volume = {15},
	Year = {2014}}

@article{Zhigulin:2004aa,
	Author = {Zhigulin, Valentin P},
	Date-Added = {2019-10-07 02:53:19 -0500},
	Date-Modified = {2019-10-07 02:53:19 -0500},
	Journal = {Physical review letters},
	Number = {23},
	Pages = {238701},
	Publisher = {APS},
	Title = {Dynamical motifs: building blocks of complex dynamics in sparsely connected random networks},
	Ty = {JOUR},
	Volume = {92},
	Year = {2004}}

@article{Hinton:2012aa,
	Author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
	Date-Added = {2019-10-07 02:51:48 -0500},
	Date-Modified = {2019-10-07 02:51:48 -0500},
	Journal = {arXiv preprint arXiv:1207.0580},
	Title = {Improving neural networks by preventing co-adaptation of feature detectors},
	Ty = {JOUR},
	Year = {2012}}

@article{Mocanu:2018aa,
	Author = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
	Date-Added = {2019-10-07 02:51:09 -0500},
	Date-Modified = {2019-10-07 02:51:09 -0500},
	Isbn = {2041-1723},
	Journal = {Nature communications},
	Number = {1},
	Pages = {2383},
	Publisher = {Nature Publishing Group},
	Title = {Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
	Ty = {JOUR},
	Volume = {9},
	Year = {2018}}

@inproceedings{Wan:2013aa,
	Author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
	Date-Added = {2019-10-07 02:49:40 -0500},
	Date-Modified = {2019-10-07 02:49:40 -0500},
	Journal = {International conference on machine learning},
	Pages = {1058--1066},
	Title = {Regularization of neural networks using dropconnect},
	Ty = {CONF},
	Year = {2013}}

@article{Shazeer:2017aa,
	Author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	Date-Added = {2019-10-07 02:47:58 -0500},
	Date-Modified = {2019-10-07 02:47:58 -0500},
	Journal = {arXiv preprint arXiv:1701.06538},
	Title = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
	Ty = {JOUR},
	Year = {2017}}

@article{Jacobs_1991,
	Author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
	Date-Added = {2019-10-07 02:43:52 -0500},
	Date-Modified = {2019-10-07 02:43:52 -0500},
	Doi = {10.1162/neco.1991.3.1.79},
	Issn = {1530-888X},
	Journal = {Neural Computation},
	Month = {Feb},
	Number = {1},
	Pages = {79--87},
	Publisher = {MIT Press - Journals},
	Title = {Adaptive Mixtures of Local Experts},
	Url = {http://dx.doi.org/10.1162/neco.1991.3.1.79},
	Volume = {3},
	Year = {1991},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/neco.1991.3.1.79}}

@article{Cayco-Gajic:2017aa,
	Abstract = {Pattern separation is a fundamental function of the brain. The divergent feedforward networks thought to underlie this computation are widespread, yet exhibit remarkably similar sparse synaptic connectivity. Marr-Albus theory postulates that such networks separate overlapping activity patterns by mapping them onto larger numbers of sparsely active neurons. But spatial correlations in synaptic input and those introduced by network connectivity are likely to compromise performance. To investigate the structural and functional determinants of pattern separation we built models of the cerebellar input layer with spatially correlated input patterns, and systematically varied their synaptic connectivity. Performance was quantified by the learning speed of a classifier trained on either the input or output patterns. Our results show that sparse synaptic connectivity is essential for separating spatially correlated input patterns over a wide range of network activity, and that expansion and correlations, rather than sparse activity, are the major determinants of pattern separation.},
	Author = {Cayco-Gajic, N. Alex and Clopath, Claudia and Silver, R. Angus},
	Da = {2017/10/24},
	Date-Added = {2019-10-07 02:39:02 -0500},
	Date-Modified = {2019-10-07 02:39:02 -0500},
	Doi = {10.1038/s41467-017-01109-y},
	Id = {Cayco-Gajic2017},
	Isbn = {2041-1723},
	Journal = {Nature Communications},
	Number = {1},
	Pages = {1116},
	Title = {Sparse synaptic connectivity is required for decorrelation and pattern separation in feedforward networks},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41467-017-01109-y},
	Volume = {8},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41467-017-01109-y}}

@article{SPORNS_2004,
	Author = {SPORNS, O and CHIALVO, D and KAISER, M and HILGETAG, C},
	Date-Added = {2019-10-07 02:37:28 -0500},
	Date-Modified = {2019-10-07 02:37:28 -0500},
	Doi = {10.1016/j.tics.2004.07.008},
	Issn = {1364-6613},
	Journal = {Trends in Cognitive Sciences},
	Month = {Sep},
	Number = {9},
	Pages = {418--425},
	Publisher = {Elsevier BV},
	Title = {Organization, development and function of complex brain networks},
	Url = {http://dx.doi.org/10.1016/j.tics.2004.07.008},
	Volume = {8},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.tics.2004.07.008}}

@article{DBLP:journals/corr/FernandoBBZHRPW17,
	Archiveprefix = {arXiv},
	Author = {Chrisantha Fernando and Dylan Banarse and Charles Blundell and Yori Zwols and David Ha and Andrei A. Rusu and Alexander Pritzel and Daan Wierstra},
	Bibsource = {dblp computer science bibliography, https://dblp.org},
	Biburl = {https://dblp.org/rec/bib/journals/corr/FernandoBBZHRPW17},
	Date-Added = {2019-10-07 02:35:55 -0500},
	Date-Modified = {2019-10-07 02:35:55 -0500},
	Eprint = {1701.08734},
	Journal = {CoRR},
	Timestamp = {Mon, 13 Aug 2018 16:49:06 +0200},
	Title = {PathNet: Evolution Channels Gradient Descent in Super Neural Networks},
	Url = {http://arxiv.org/abs/1701.08734},
	Volume = {abs/1701.08734},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1701.08734}}

@incollection{NIPS2015_5784,
	Author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
	Booktitle = {Advances in Neural Information Processing Systems 28},
	Date-Added = {2019-10-07 02:28:48 -0500},
	Date-Modified = {2019-10-07 02:28:48 -0500},
	Editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	Pages = {1135--1143},
	Publisher = {Curran Associates, Inc.},
	Title = {Learning both Weights and Connections for Efficient Neural Network},
	Url = {http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf}}

@article{Bassett:2006aa,
	Annote = {doi: 10.1177/1073858406293182},
	Author = {Bassett, Danielle Smith and Bullmore, Ed},
	Booktitle = {The Neuroscientist},
	Da = {2006/12/01},
	Date = {2006/12/01},
	Date-Added = {2019-10-07 02:27:49 -0500},
	Date-Modified = {2019-10-07 02:27:49 -0500},
	Doi = {10.1177/1073858406293182},
	Isbn = {1073-8584},
	Journal = {The Neuroscientist},
	Journal1 = {Neuroscientist},
	M3 = {doi: 10.1177/1073858406293182},
	Month = {2019/10/07},
	N2 = {Many complex networks have a small-world topology characterized by dense local clustering or cliquishness of connections between neighboring nodes yet a short path length between any (distant) pair of nodes due to the existence of relatively few long-range connections. This is an attractive model for the organization of brain anatomical and functional networks because a small-world topology can support both segregated/specialized and distributed/integrated information processing. Moreover, small-world networks are economical, tending to minimize wiring costs while supporting high dynamical complexity. The authors introduce some of the key mathematical concepts in graph theory required for small-world analysis and review how these methods have been applied to quantification of cortical connectivity matrices derived from anatomical tract-tracing studies in the macaque monkey and the cat. The evolution of small-world networks is discussed in terms of a selection pressure to deliver cost-effective information-processing systems. The authors illustrate how these techniques and concepts are increasingly being applied to the analysis of human brain functional networks derived from electroencephalography/magnetoencephalography and fMRI experiments. Finally, the authors consider the relevance of small-world models for understanding the emergence of complex behaviors and the resilience of brain systems to pathological attack by disease or aberrant development. They conclude that small-world models provide a powerful and versatile approach to understanding the structure and function of human brain systems.},
	Number = {6},
	Pages = {512--523},
	Publisher = {SAGE Publications Inc STM},
	Title = {Small-World Brain Networks},
	Ty = {JOUR},
	Url = {https://doi.org/10.1177/1073858406293182},
	Volume = {12},
	Year = {2006},
	Year1 = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1177/1073858406293182}}

@article{cite-key,
	Abstract = {Through the success of deep learning in various domains, artificial neural networks are currently among the most used artificial intelligence methods. Taking inspiration from the network properties of biological neural networks (e.g. sparsity, scale-freeness), we argue that (contrary to general practice) artificial neural networks, too, should not have fully-connected layers. Here we propose sparse evolutionary training of artificial neural networks, an algorithm which evolves an initial sparse topology (Erd{\H o}s--R{\'e}nyi random graph) of two consecutive layers of neurons into a scale-free topology, during learning. Our method replaces artificial neural networks fully-connected layers with sparse ones before training, reducing quadratically the number of parameters, with no decrease in accuracy. We demonstrate our claims on restricted Boltzmann machines, multi-layer perceptrons, and convolutional neural networks for unsupervised and supervised learning on 15 datasets. Our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible.},
	Author = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
	Da = {2018/06/19},
	Date-Added = {2019-10-07 02:22:50 -0500},
	Date-Modified = {2019-10-07 02:22:50 -0500},
	Doi = {10.1038/s41467-018-04316-3},
	Id = {Mocanu2018},
	Isbn = {2041-1723},
	Journal = {Nature Communications},
	Number = {1},
	Pages = {2383},
	Title = {Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41467-018-04316-3},
	Volume = {9},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41467-018-04316-3}}

@misc{kingma2013autoencoding,
	Archiveprefix = {arXiv},
	Author = {Diederik P Kingma and Max Welling},
	Eprint = {1312.6114},
	Primaryclass = {stat.ML},
	Title = {Auto-Encoding Variational Bayes},
	Year = {2013}}
	
@inproceedings{Krizhevsky2012ImageNetCW,
  title={ImageNet Classification with Deep Convolutional Neural Networks},
  author={Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  booktitle={NIPS},
  year={2012}
}