% Encoding: UTF-8
%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Daniel at 2020-04-14 00:03:17 -0500 


%% Saved with string encoding Unicode (UTF-8)

@Article{doi:10.1162/neco.1997.9.1.185,
  author        = {Setiono, Rudy},
  journal       = {Neural Computation},
  title         = {A Penalty-Function Approach for Pruning Feedforward Neural Networks},
  year          = {1997},
  number        = {1},
  pages         = {185-204},
  volume        = {9},
  abstract      = {This article proposes the use of a penalty function for pruning feedforward neural network by weight elimination. The penalty function proposed consists of two terms. The first term is to discourage the use of unnecessary connections, and the second term is to prevent the weights of the connections from taking excessively large values. Simple criteria for eliminating weights from the network are also given. The effectiveness of this penalty function is tested on three well-known problems: the contiguity problem, the parity problems, and the monks problems. The resulting pruned networks obtained for many of these problems have fewer connections than previously reported in the literature.},
  bdsk-url-1    = {https://doi.org/10.1162/neco.1997.9.1.185},
  date-added    = {2020-04-14 00:03:02 -0500},
  date-modified = {2020-04-14 00:03:02 -0500},
  doi           = {10.1162/neco.1997.9.1.185},
  eprint        = {https://doi.org/10.1162/neco.1997.9.1.185},
  url           = {https://doi.org/10.1162/neco.1997.9.1.185},
}

@book{Kingma:2015aa,
	Author = {Diederik P. Kingma},
	Date-Added = {2020-04-13 23:54:46 -0500},
	Date-Modified = {2020-04-13 23:54:46 -0500},
	Title = {Adam: A Method for Stochastic Optimization.; ICLR (Poster)},
	Url = {http://arxiv.org/abs/1412.6980},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1412.6980}}

@article{Louizos2017LearningSN,
	Author = {Christos Louizos and Max Welling and Diederik P. Kingma},
	Date-Added = {2020-04-13 23:14:10 -0500},
	Date-Modified = {2020-04-13 23:14:10 -0500},
	Journal = {ArXiv},
	Title = {Learning Sparse Neural Networks through L0 Regularization},
	Volume = {abs/1712.01312},
	Year = {2017}}

@article{Hershey_2007,
	Author = {Hershey, John R. and Olsen, Peder A.},
	Date-Added = {2020-04-13 23:11:42 -0500},
	Date-Modified = {2020-04-13 23:11:42 -0500},
	Doi = {10.1109/icassp.2007.366913},
	Isbn = {1424407273},
	Journal = {2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07},
	Month = {Apr},
	Publisher = {IEEE},
	Title = {Approximating the Kullback Leibler Divergence Between Gaussian Mixture Models},
	Url = {http://dx.doi.org/10.1109/ICASSP.2007.366913},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICASSP.2007.366913},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/icassp.2007.366913}}

@article{Mitchell_1988,
	Author = {Mitchell, T. J. and Beauchamp, J. J.},
	Date-Added = {2020-04-13 23:11:41 -0500},
	Date-Modified = {2020-04-13 23:11:41 -0500},
	Doi = {10.1080/01621459.1988.10478694},
	Issn = {1537-274X},
	Journal = {Journal of the American Statistical Association},
	Month = {Dec},
	Number = {404},
	Pages = {1023--1032},
	Publisher = {Informa UK Limited},
	Title = {Bayesian Variable Selection in Linear Regression},
	Url = {http://dx.doi.org/10.1080/01621459.1988.10478694},
	Volume = {83},
	Year = {1988},
	Bdsk-Url-1 = {http://dx.doi.org/10.1080/01621459.1988.10478694}}

@article{Caswell:2016aa,
	Author = {Caswell, Isaac and Shen, Chuanqi and Wang, Lisa},
	Date-Added = {2019-10-07 03:14:13 -0500},
	Date-Modified = {2019-10-07 03:14:13 -0500},
	Journal = {Tech. Report},
	Title = {Loopy neural nets: Imitating feedback loops in the human brain},
	Ty = {JOUR},
	Year = {2016}}

@inproceedings{Krizhevsky:2011aa,
	Author = {Krizhevsky, Alex and Hinton, Geoffrey E},
	Date-Added = {2019-10-07 03:11:10 -0500},
	Date-Modified = {2019-10-07 03:11:10 -0500},
	Journal = {ESANN},
	Pages = {2},
	Title = {Using very deep autoencoders for content-based image retrieval.},
	Ty = {CONF},
	Volume = {1},
	Year = {2011}}

@article{Sporns:2004aa,
	Author = {Sporns, Olaf and K{\"o}tter, Rolf},
	Date-Added = {2019-10-07 03:06:08 -0500},
	Date-Modified = {2019-10-07 03:06:08 -0500},
	Isbn = {1545-7885},
	Journal = {PLoS biology},
	Number = {11},
	Pages = {e369},
	Publisher = {Public Library of Science},
	Title = {Motifs in brain networks},
	Ty = {JOUR},
	Volume = {2},
	Year = {2004}}

@inproceedings{Kiyono:2019aa,
	Author = {Kiyono, Shun and Suzuki, Jun and Inui, Kentaro},
	Date-Added = {2019-10-07 03:03:53 -0500},
	Date-Modified = {2019-10-07 03:03:53 -0500},
	Isbn = {2374-3468},
	Journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	Pages = {4073--4081},
	Title = {Mixture of expert/imitator networks: Scalable semi-supervised learning framework},
	Ty = {CONF},
	Volume = {33},
	Year = {2019}}

@inproceedings{Hassibi:1993aa,
	Author = {Hassibi, Babak and Stork, David G and Wolff, Gregory J},
	Date-Added = {2019-10-07 03:01:19 -0500},
	Date-Modified = {2019-10-07 03:01:19 -0500},
	Isbn = {0780309995},
	Journal = {IEEE international conference on neural networks},
	Pages = {293--299},
	Publisher = {IEEE},
	Title = {Optimal brain surgeon and general network pruning},
	Ty = {CONF},
	Year = {1993}}

@inproceedings{LeCun:1990aa,
	Author = {LeCun, Yann and Denker, John S and Solla, Sara A},
	Date-Added = {2019-10-07 03:00:27 -0500},
	Date-Modified = {2019-10-07 03:00:27 -0500},
	Journal = {Advances in neural information processing systems},
	Pages = {598--605},
	Title = {Optimal brain damage},
	Ty = {CONF},
	Year = {1990}}

@inproceedings{Zorins:2015aa,
	Author = {Zorins, Aleksejs and Grabusts, Peter},
	Date-Added = {2019-10-07 03:00:04 -0500},
	Date-Modified = {2019-10-07 03:00:04 -0500},
	Journal = {Proceedings of the 10th International Scientific and Practical Conference. Volume III},
	Pages = {231},
	Title = {Artificial Neural Networks and Human Brain: Survey of Improvement Possibilities of Learning},
	Ty = {CONF},
	Volume = {228},
	Year = {2015}}

@article{Samsonovich:2005aa,
	Author = {Samsonovich, Alexei V and Ascoli, Giorgio A},
	Date-Added = {2019-10-07 02:58:30 -0500},
	Date-Modified = {2019-10-07 02:58:30 -0500},
	Isbn = {1072-0502},
	Journal = {Learning \& Memory},
	Number = {2},
	Pages = {193--208},
	Publisher = {Cold Spring Harbor Lab},
	Title = {A simple neural network model of the hippocampus suggesting its pathfinding role in episodic memory retrieval},
	Ty = {JOUR},
	Volume = {12},
	Year = {2005}}

@article{Morris:2003aa,
	Author = {Morris, Genela and Nevet, Alon and Bergman, Hagai},
	Date-Added = {2019-10-07 02:57:47 -0500},
	Date-Modified = {2019-10-07 02:57:47 -0500},
	Isbn = {0928-4257},
	Journal = {Journal of Physiology-Paris},
	Number = {4-6},
	Pages = {581--589},
	Publisher = {Elsevier},
	Title = {Anatomical funneling, sparse connectivity and redundancy reduction in the neural networks of the basal ganglia},
	Ty = {JOUR},
	Volume = {97},
	Year = {2003}}

@article{Watts:1998aa,
	Author = {Watts, Duncan J and Strogatz, Steven H},
	Date-Added = {2019-10-07 02:56:10 -0500},
	Date-Modified = {2019-10-07 02:56:10 -0500},
	Isbn = {1476-4687},
	Journal = {nature},
	Number = {6684},
	Pages = {440},
	Publisher = {Nature Publishing Group},
	Title = {Collective dynamics of `small-world'networks},
	Ty = {JOUR},
	Volume = {393},
	Year = {1998}}

@article{Han:2015aa,
	Author = {Han, Song and Mao, Huizi and Dally, William J},
	Date-Added = {2019-10-07 02:55:01 -0500},
	Date-Modified = {2019-10-07 02:55:01 -0500},
	Journal = {arXiv preprint arXiv:1510.00149},
	Title = {Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
	Ty = {JOUR},
	Year = {2015}}

@inproceedings{Huang:2017aa,
	Author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
	Date-Added = {2019-10-07 02:54:12 -0500},
	Date-Modified = {2019-10-07 02:54:12 -0500},
	Journal = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	Pages = {4700--4708},
	Title = {Densely connected convolutional networks},
	Ty = {CONF},
	Year = {2017}}

@article{Srivastava:2014aa,
	Author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	Date-Added = {2019-10-07 02:53:43 -0500},
	Date-Modified = {2019-10-07 02:53:43 -0500},
	Isbn = {1532-4435},
	Journal = {The journal of machine learning research},
	Number = {1},
	Pages = {1929--1958},
	Publisher = {JMLR. org},
	Title = {Dropout: a simple way to prevent neural networks from overfitting},
	Ty = {JOUR},
	Volume = {15},
	Year = {2014}}

@article{Zhigulin:2004aa,
	Author = {Zhigulin, Valentin P},
	Date-Added = {2019-10-07 02:53:19 -0500},
	Date-Modified = {2019-10-07 02:53:19 -0500},
	Journal = {Physical review letters},
	Number = {23},
	Pages = {238701},
	Publisher = {APS},
	Title = {Dynamical motifs: building blocks of complex dynamics in sparsely connected random networks},
	Ty = {JOUR},
	Volume = {92},
	Year = {2004}}

@article{Hinton:2012aa,
	Author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
	Date-Added = {2019-10-07 02:51:48 -0500},
	Date-Modified = {2019-10-07 02:51:48 -0500},
	Journal = {arXiv preprint arXiv:1207.0580},
	Title = {Improving neural networks by preventing co-adaptation of feature detectors},
	Ty = {JOUR},
	Year = {2012}}

@article{Mocanu:2018aa,
	Author = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
	Date-Added = {2019-10-07 02:51:09 -0500},
	Date-Modified = {2019-10-07 02:51:09 -0500},
	Isbn = {2041-1723},
	Journal = {Nature communications},
	Number = {1},
	Pages = {2383},
	Publisher = {Nature Publishing Group},
	Title = {Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
	Ty = {JOUR},
	Volume = {9},
	Year = {2018}}

@inproceedings{Wan:2013aa,
	Author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
	Date-Added = {2019-10-07 02:49:40 -0500},
	Date-Modified = {2019-10-07 02:49:40 -0500},
	Journal = {International conference on machine learning},
	Pages = {1058--1066},
	Title = {Regularization of neural networks using dropconnect},
	Ty = {CONF},
	Year = {2013}}

@article{Shazeer:2017aa,
	Author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	Date-Added = {2019-10-07 02:47:58 -0500},
	Date-Modified = {2019-10-07 02:47:58 -0500},
	Journal = {arXiv preprint arXiv:1701.06538},
	Title = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
	Ty = {JOUR},
	Year = {2017}}

@article{Jacobs_1991,
	Author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
	Date-Added = {2019-10-07 02:43:52 -0500},
	Date-Modified = {2019-10-07 02:43:52 -0500},
	Doi = {10.1162/neco.1991.3.1.79},
	Issn = {1530-888X},
	Journal = {Neural Computation},
	Month = {Feb},
	Number = {1},
	Pages = {79--87},
	Publisher = {MIT Press - Journals},
	Title = {Adaptive Mixtures of Local Experts},
	Url = {http://dx.doi.org/10.1162/neco.1991.3.1.79},
	Volume = {3},
	Year = {1991},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/neco.1991.3.1.79}}

@article{Cayco-Gajic:2017aa,
	Abstract = {Pattern separation is a fundamental function of the brain. The divergent feedforward networks thought to underlie this computation are widespread, yet exhibit remarkably similar sparse synaptic connectivity. Marr-Albus theory postulates that such networks separate overlapping activity patterns by mapping them onto larger numbers of sparsely active neurons. But spatial correlations in synaptic input and those introduced by network connectivity are likely to compromise performance. To investigate the structural and functional determinants of pattern separation we built models of the cerebellar input layer with spatially correlated input patterns, and systematically varied their synaptic connectivity. Performance was quantified by the learning speed of a classifier trained on either the input or output patterns. Our results show that sparse synaptic connectivity is essential for separating spatially correlated input patterns over a wide range of network activity, and that expansion and correlations, rather than sparse activity, are the major determinants of pattern separation.},
	Author = {Cayco-Gajic, N. Alex and Clopath, Claudia and Silver, R. Angus},
	Da = {2017/10/24},
	Date-Added = {2019-10-07 02:39:02 -0500},
	Date-Modified = {2019-10-07 02:39:02 -0500},
	Doi = {10.1038/s41467-017-01109-y},
	Id = {Cayco-Gajic2017},
	Isbn = {2041-1723},
	Journal = {Nature Communications},
	Number = {1},
	Pages = {1116},
	Title = {Sparse synaptic connectivity is required for decorrelation and pattern separation in feedforward networks},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41467-017-01109-y},
	Volume = {8},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41467-017-01109-y}}

@article{SPORNS_2004,
	Author = {SPORNS, O and CHIALVO, D and KAISER, M and HILGETAG, C},
	Date-Added = {2019-10-07 02:37:28 -0500},
	Date-Modified = {2019-10-07 02:37:28 -0500},
	Doi = {10.1016/j.tics.2004.07.008},
	Issn = {1364-6613},
	Journal = {Trends in Cognitive Sciences},
	Month = {Sep},
	Number = {9},
	Pages = {418--425},
	Publisher = {Elsevier BV},
	Title = {Organization, development and function of complex brain networks},
	Url = {http://dx.doi.org/10.1016/j.tics.2004.07.008},
	Volume = {8},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.tics.2004.07.008}}

@article{DBLP:journals/corr/FernandoBBZHRPW17,
	Archiveprefix = {arXiv},
	Author = {Chrisantha Fernando and Dylan Banarse and Charles Blundell and Yori Zwols and David Ha and Andrei A. Rusu and Alexander Pritzel and Daan Wierstra},
	Bibsource = {dblp computer science bibliography, https://dblp.org},
	Biburl = {https://dblp.org/rec/bib/journals/corr/FernandoBBZHRPW17},
	Date-Added = {2019-10-07 02:35:55 -0500},
	Date-Modified = {2019-10-07 02:35:55 -0500},
	Eprint = {1701.08734},
	Journal = {CoRR},
	Timestamp = {Mon, 13 Aug 2018 16:49:06 +0200},
	Title = {PathNet: Evolution Channels Gradient Descent in Super Neural Networks},
	Url = {http://arxiv.org/abs/1701.08734},
	Volume = {abs/1701.08734},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1701.08734}}

@incollection{NIPS2015_5784,
	Author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
	Booktitle = {Advances in Neural Information Processing Systems 28},
	Date-Added = {2019-10-07 02:28:48 -0500},
	Date-Modified = {2019-10-07 02:28:48 -0500},
	Editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	Pages = {1135--1143},
	Publisher = {Curran Associates, Inc.},
	Title = {Learning both Weights and Connections for Efficient Neural Network},
	Url = {http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf}}

@article{Bassett:2006aa,
	Annote = {doi: 10.1177/1073858406293182},
	Author = {Bassett, Danielle Smith and Bullmore, Ed},
	Booktitle = {The Neuroscientist},
	Da = {2006/12/01},
	Date = {2006/12/01},
	Date-Added = {2019-10-07 02:27:49 -0500},
	Date-Modified = {2019-10-07 02:27:49 -0500},
	Doi = {10.1177/1073858406293182},
	Isbn = {1073-8584},
	Journal = {The Neuroscientist},
	Journal1 = {Neuroscientist},
	M3 = {doi: 10.1177/1073858406293182},
	Month = {2019/10/07},
	N2 = {Many complex networks have a small-world topology characterized by dense local clustering or cliquishness of connections between neighboring nodes yet a short path length between any (distant) pair of nodes due to the existence of relatively few long-range connections. This is an attractive model for the organization of brain anatomical and functional networks because a small-world topology can support both segregated/specialized and distributed/integrated information processing. Moreover, small-world networks are economical, tending to minimize wiring costs while supporting high dynamical complexity. The authors introduce some of the key mathematical concepts in graph theory required for small-world analysis and review how these methods have been applied to quantification of cortical connectivity matrices derived from anatomical tract-tracing studies in the macaque monkey and the cat. The evolution of small-world networks is discussed in terms of a selection pressure to deliver cost-effective information-processing systems. The authors illustrate how these techniques and concepts are increasingly being applied to the analysis of human brain functional networks derived from electroencephalography/magnetoencephalography and fMRI experiments. Finally, the authors consider the relevance of small-world models for understanding the emergence of complex behaviors and the resilience of brain systems to pathological attack by disease or aberrant development. They conclude that small-world models provide a powerful and versatile approach to understanding the structure and function of human brain systems.},
	Number = {6},
	Pages = {512--523},
	Publisher = {SAGE Publications Inc STM},
	Title = {Small-World Brain Networks},
	Ty = {JOUR},
	Url = {https://doi.org/10.1177/1073858406293182},
	Volume = {12},
	Year = {2006},
	Year1 = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1177/1073858406293182}}

@Article{cite-key,
  author        = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
  journal       = {Nature Communications},
  title         = {Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  year          = {2018},
  number        = {1},
  pages         = {2383},
  volume        = {9},
  abstract      = {Through the success of deep learning in various domains, artificial neural networks are currently among the most used artificial intelligence methods. Taking inspiration from the network properties of biological neural networks (e.g. sparsity, scale-freeness), we argue that (contrary to general practice) artificial neural networks, too, should not have fully-connected layers. Here we propose sparse evolutionary training of artificial neural networks, an algorithm which evolves an initial sparse topology (Erd{\H o}s--R{\'e}nyi random graph) of two consecutive layers of neurons into a scale-free topology, during learning. Our method replaces artificial neural networks fully-connected layers with sparse ones before training, reducing quadratically the number of parameters, with no decrease in accuracy. We demonstrate our claims on restricted Boltzmann machines, multi-layer perceptrons, and convolutional neural networks for unsupervised and supervised learning on 15 datasets. Our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible.},
  bdsk-url-1    = {https://doi.org/10.1038/s41467-018-04316-3},
  da            = {2018/06/19},
  date-added    = {2019-10-07 02:22:50 -0500},
  date-modified = {2019-10-07 02:22:50 -0500},
  doi           = {10.1038/s41467-018-04316-3},
  id            = {Mocanu2018},
  isbn          = {2041-1723},
  ty            = {JOUR},
  url           = {https://doi.org/10.1038/s41467-018-04316-3},
}

@misc{kingma2013autoencoding,
	Archiveprefix = {arXiv},
	Author = {Diederik P Kingma and Max Welling},
	Eprint = {1312.6114},
	Primaryclass = {stat.ML},
	Title = {Auto-Encoding Variational Bayes},
	Year = {2013}}
	
@inproceedings{Krizhevsky2012ImageNetCW,
  title={ImageNet Classification with Deep Convolutional Neural Networks},
  author={Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  booktitle={NIPS},
  year={2012}
}
@Article{Tan2019,
  author       = {Mingxing Tan and Quoc V. Le},
  title        = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  abstract     = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  date         = {2019-05-28},
  eprint       = {1905.11946v3},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1905.11946v3:PDF},
  journaltitle = {ICML 2019},
  keywords     = {cs.LG, cs.CV, stat.ML},
}

@Article{Han2015,
  author      = {Song Han and Huizi Mao and William J. Dally},
  title       = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  abstract    = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
  date        = {2015-10-01},
  eprint      = {1510.00149v5},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1510.00149v5:PDF},
  keywords    = {cs.CV, cs.NE},
}

@Manual{Jonnalagadda:2018,
  title   = {Sparse, Stacked and Variational Autoencoder},
  author  = {Venkata Krishna Jonnalagadda},
  year    = {2018},
  url     = {https://medium.com/@venkatakrishna.jonnalagadda/sparse-stacked-and-variational-autoencoder-efe5bfe73b64},
  urldate = {2018-01-06},
}

@Manual{Trueman:2019,
  title   = {Why data centres are the new frontier in the fight against climate change},
  author  = {Charlotte Trueman},
  year    = {2019},
  url     = {https://www.computerworld.com/article/3431148/why-data-centres-are-the-new-frontier-in-the-fight-against-climate-change.html},
  urldate = {2019-08-09},
}

@Manual{Sattiraju:2020,
  title   = {Google Data Centersâ€™ Secret Cost: Billions of Gallons of Water},
  author  = {Nikitha Sattiraju},
  year    = {2020},
  url     = {https://www.bloomberg.com/news/features/2020-04-01/how-much-water-do-google-data-centers-use-billions-of-gallons},
  urldate = {2020-01-2020},
}

@Article{Zeiler2013,
  author      = {Matthew D Zeiler and Rob Fergus},
  title       = {Visualizing and Understanding Convolutional Networks},
  abstract    = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  date        = {2013-11-12},
  eprint      = {1311.2901v3},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1311.2901v3:PDF},
  keywords    = {cs.CV},
}

@Article{,
}

@Comment{jabref-meta: databaseType:bibtex;}
